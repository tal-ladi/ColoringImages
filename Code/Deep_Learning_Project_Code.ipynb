{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Learning Project Code.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6Z8gQVsv7mQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" # Commands to download datset directly from colab\n",
        "from google.colab import files\n",
        "files.upload() # upload kaggle.json file\n",
        "!pip install kaggle\n",
        "!mkdir -p ~/.kaggle/\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d nitishabharathi/scene-classification\n",
        "!mkdir Dataset\n",
        "!unzip scene-classification.zip -d Dataset\n",
        "\"\"\"\n",
        "# Imports\n",
        "import pickle\n",
        "import numpy as np \n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Input, Conv2D, Add, Conv2DTranspose, MaxPooling2D, concatenate, Flatten, Dense\n",
        "from PIL import Image\n",
        "\n",
        "# Constants\n",
        "image_size = (256, 256)  # power of two recommended for downsampling\n",
        "batch_size = 1  # SGD \n",
        "\n",
        "# Dataset Function Definitions\n",
        "\n",
        "def download_dataset_files(folderpath: str, ext: str):\n",
        "  \"\"\"\n",
        "  filepath: str:: full or incomplete path to dataset folder.\n",
        "  ext: desired extention, for images it probably is .jpg or .png.\n",
        "  \"\"\"\n",
        "  image_files = []\n",
        "  for folder, _, filenames in os.walk(folderpath):\n",
        "    for filename in filenames:\n",
        "      full_path = os.path.join(folder, filename)\n",
        "      if full_path.endswith(ext):\n",
        "        image_files.append(full_path)\n",
        "  return np.array(image_files)\n",
        "  \n",
        "\n",
        "def filter_and_print(paths: str):\n",
        "  \"\"\"\n",
        "  Check if path is infact an image that can be opened\n",
        "  into python.\n",
        "  \"\"\"\n",
        "  valid_paths = []\n",
        "  for path in paths:\n",
        "    try:\n",
        "      image = Image.open(path)\n",
        "      image.verify()\n",
        "      valid_paths.append(path)\n",
        "    except Exception as e:\n",
        "      print(\"couldn't open file {}, got error {}\".format(path, e))\n",
        "  return valid_paths\n",
        "\n",
        "def read_image_tf(file):\n",
        "  output = tf.io.read_file(file)\n",
        "  output = tf.image.decode_jpeg(output, channels=3)\n",
        "  output = tf.image.resize(output, image_size)\n",
        "  return output\n",
        "\n",
        "def rgb_to_grayscale(image):\n",
        "    grayscale_image = tf.image.rgb_to_grayscale(image)\n",
        "    grayscale_image_normalized = tf.math.divide(grayscale_image, 255)\n",
        "    return grayscale_image_normalized\n",
        "\n",
        "def download_dataset(paths):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(paths)\n",
        "    rgb_images = dataset.map(read_image_tf)\n",
        "    grayscale_images = rgb_images.map(rgb_to_grayscale)\n",
        "    dataset = tf.data.Dataset.zip((grayscale_images, rgb_images))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE) #  optimize pre-fetching off data to speed up computation\n",
        "    return dataset\n",
        "\n",
        "# Model class Definitions\n",
        "\n",
        "class ImageColorizerBase(object):\n",
        "  def __init__(self, model):\n",
        "    self.model = model\n",
        "    self.history = None\n",
        "  \n",
        "  def save_results(self, name):\n",
        "    self.weights_file = name + '.h5'\n",
        "    self.history_file = name + '_history'\n",
        "\n",
        "    try:\n",
        "      self.model.save(self.weights_file)\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "    try:\n",
        "      with open(self.history_file, 'wb') as file:\n",
        "        pickle.dump(self.history.history, file)\n",
        "    except:\n",
        "      pass\n",
        "    \n",
        "    try:\n",
        "      files.download(self.weights_file)\n",
        "    except:\n",
        "      pass\n",
        "    \n",
        "    try:\n",
        "      files.download(self.history_file)\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "  def fit(self, train, validation, niter=1):\n",
        "\n",
        "    physical_devices = tf.config.list_physical_devices('GPU')\n",
        "\n",
        "    if len(physical_devices)>0:\n",
        "      device=\"/GPU:0\"\n",
        "    else:\n",
        "      device=\"/CPU:0\"\n",
        "    \n",
        "    with tf.device(device):\n",
        "      self.history = self.model.fit(train,\n",
        "              validation_data=validation,\n",
        "                epochs=niter)\n",
        "\n",
        "class UnetRegressor(ImageColorizerBase):\n",
        "  def __init__(self):\n",
        "    inputs = Input(image_size + (1,))\n",
        "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n",
        "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n",
        "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)\n",
        "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
        "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)\n",
        "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)\n",
        "\n",
        "    up6 = concatenate([Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv5), conv4], axis=3)\n",
        "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)\n",
        "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\n",
        "    up7 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv6), conv3], axis=3)\n",
        "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)\n",
        "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\n",
        "    up8 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv7), conv2], axis=3)\n",
        "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)\n",
        "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)\n",
        "    up9 = concatenate([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv8), conv1], axis=3)\n",
        "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)\n",
        "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)\n",
        "    conv10 = Conv2D(3, (3, 3), activation='relu', padding='same')(conv9)\n",
        "\n",
        "    model = Model(inputs=[inputs], outputs=[conv10])\n",
        "    model.compile(optimizer = tf.keras.optimizers.Adam(), loss ='MSE')\n",
        "\n",
        "    super().__init__(model)\n",
        "  \n",
        "if __name__ == \"__main__\":\n",
        "  # fetch full paths to images\n",
        "  paths = download_dataset_files('Dataset', '.jpg')\n",
        "  paths = filter_and_print(paths)\n",
        "\n",
        "  # split paths to train and test image files\n",
        "  train_percentage = 0.8\n",
        "  cutoff = int(train_percentage*len(paths))\n",
        "  train_paths = paths[:cutoff]\n",
        "  test_paths = paths[cutoff:]\n",
        "\n",
        "  # download train and test datasets as tf dataset object\n",
        "  train_dataset = download_dataset(train_paths)\n",
        "  test_dataset = download_dataset(test_paths)\n",
        "\n",
        "  model = UnetRegressor()\n",
        "  model.fit(train_dataset, test_dataset, niter=30)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}